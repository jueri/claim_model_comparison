{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "micGWFxfsDZ4"
      },
      "source": [
        "# fasttext\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPwkDUcWjGBG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from config import DATASETS, FASTTEXT_PATH, PROJECT_NAME\n",
        "from nltk.corpus import stopwords  # type: ignore\n",
        "from nltk.stem import WordNetLemmatizer  # type: ignore\n",
        "from sklearn.base import BaseEstimator  # type: ignore\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls53eggzhsZf"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"fasttext\"\n",
        "TRACKING = True\n",
        "FALSE_CLASS_BALANCE = 1.0\n",
        "\n",
        "dataset = DATASETS[\"dataset_2014\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfyOhWODhsZg"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_path: str, split_size: float=0.2, false_class_balance: float=1.0) -> pd.DataFrame:\n",
        "    \"\"\"Function to load the dataset.\n",
        "\n",
        "    Returns:\n",
        "        X_train (DatFrame): Train data\n",
        "        X_test (DatFrame): Test data\n",
        "        y_train (DatFrame): Train label\n",
        "        y_test (DatFrame): Test label\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(os.path.join(dataset_path))  # load Data\n",
        "\n",
        "    claims = data[data[\"Claim\"] == True]\n",
        "\n",
        "    n_samples = int(len(claims) * false_class_balance)\n",
        "    no_claims = data[data[\"Claim\"] == False].sample(n=n_samples, random_state=42)\n",
        "    data_sample = pd.concat([claims, no_claims])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data_sample, data_sample[\"Claim\"], test_size=split_size, random_state=0\n",
        "    )\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPgHeKE5hsZi"
      },
      "outputs": [],
      "source": [
        "class FastTextPreprocessing(BaseEstimator):\n",
        "    \"\"\"Prepare the dataset for fasttext\"\"\"\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        return [self.__class__.__name__]\n",
        "\n",
        "    def fasttext_preprocessing(self, document):\n",
        "        \"\"\"Preprocessing pipeline from: https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/\"\"\"\n",
        "        document = re.sub(r'\\W', ' ', str(document))  # Remove all the special characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)  # remove all single characters\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)  # Remove single characters from the start\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)  # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'^b\\s+', '', document)  # Removing prefixed 'b'\n",
        "        document = document.lower()  # Converting to Lowercase\n",
        "\n",
        "        en_stop = set(stopwords.words('english'))\n",
        "        \n",
        "        # Lemmatization\n",
        "        tokens = document.split()\n",
        "        tokens = [self.stemmer.lemmatize(word) for word in tokens]\n",
        "        tokens = [word for word in tokens if word not in en_stop]\n",
        "        tokens = [word for word in tokens if len(word) > 3]\n",
        "\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "        return preprocessed_text\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        self.stemmer = WordNetLemmatizer()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y, name):\n",
        "        path = os.path.join(FASTTEXT_PATH, \"dataset_\" + name + \".txt\")\n",
        "        with open(path, 'w', encoding='utf-8') as outFile:\n",
        "            for sentence, label in zip(X, y):\n",
        "                preprcessed_sentence = self.fasttext_preprocessing(sentence)\n",
        "                preprcessed_label = \"__label__claim\" if label == True else \"__label__no_claim\"\n",
        "\n",
        "                processed_data = preprcessed_label + \" \" + preprcessed_sentence\n",
        "\n",
        "                outFile.write(processed_data)\n",
        "                outFile.write(\"\\n\")\n",
        "\n",
        "        return path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AjEyJbyhsZk"
      },
      "source": [
        "### 0. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqtIRjDxhsZl"
      },
      "outputs": [],
      "source": [
        "train_text_split, X_test, train_labels_split, y_test = load_dataset(dataset_path=os.path.join(dataset[\"base_path\"], dataset[\"data\"]), false_class_balance=FALSE_CLASS_BALANCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT9n4FtjhsZm"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(train_text_split, train_labels_split, test_size=.2, random_state=42) # train/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2DnlsewhsZn"
      },
      "source": [
        "### 1. Encode Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkNrrXhNhsZn"
      },
      "outputs": [],
      "source": [
        "text_feature = FastTextPreprocessing()\n",
        "text_feature = text_feature.fit(None, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pVwfgwahsZo"
      },
      "outputs": [],
      "source": [
        "train_data_path = text_feature.transform(\n",
        "    X_train[\"Sentence\"].to_list(),\n",
        "    y_train.to_list(), \n",
        "    \"train\"\n",
        ")\n",
        "validation_data_path = text_feature.transform(\n",
        "    X_train[\"Sentence\"].to_list(),\n",
        "    y_train.to_list(), \n",
        "    \"validate\"\n",
        ")\n",
        "\n",
        "test_data_path = text_feature.transform(\n",
        "    X_test[\"Sentence\"].to_list(),\n",
        "    y_test.to_list(), \n",
        "    \"test\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5COqsf2hsZo"
      },
      "source": [
        "### 2. Train Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkm-2qRZypqn"
      },
      "outputs": [],
      "source": [
        "model = fasttext.train_unsupervised(train_data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg7exJgdhsZq"
      },
      "source": [
        "### 3. Train classifyer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHjuHPW1hsZr"
      },
      "outputs": [],
      "source": [
        "with open(train_data_path, \"r\") as inFile:\n",
        "   len_train = len(inFile.readlines())\n",
        "with open(validation_data_path, \"r\") as inFile:\n",
        "   len_val = len(inFile.readlines())\n",
        "with open(test_data_path, \"r\") as inFile:\n",
        "   len_test = len(inFile.readlines())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vYck2TxhsZr"
      },
      "outputs": [],
      "source": [
        "model = fasttext.train_supervised(input=train_data_path, autotuneValidationFile=validation_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDBHzKDZqSZv"
      },
      "outputs": [],
      "source": [
        "if TRACKING:\n",
        "  wandb.init(project=PROJECT_NAME,\n",
        "          config={\n",
        "              \"model\": MODEL_NAME,\n",
        "              \"setup\": \"autotuneValidation\",\n",
        "              \"dataset\": dataset[\"name\"],\n",
        "              \"train_data_size\": len_train,\n",
        "              \"validation_data_size\": len_val,\n",
        "              \"test_data_size\": len_test,\n",
        "              \"batch_size\": None,\n",
        "              \"learning_rate\": model.lr,\n",
        "              \"epochs\": model.epoch,\n",
        "              \"false_class_balance\": FALSE_CLASS_BALANCE\n",
        "          })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx7MEQ8LhsZs"
      },
      "source": [
        "### 4. Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lfwtOsHhsZs"
      },
      "outputs": [],
      "source": [
        "model.test(test_data_path)  # n, precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5isYnS-nhsZs"
      },
      "outputs": [],
      "source": [
        "sentences = X_test[\"Sentence\"].to_list()\n",
        "labels = y_test.to_list()\n",
        "y_pred = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  label, confidence = model.predict(sentence)\n",
        "  y_pred.append(False if label[0]== \"__label__no_claim\" else True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN1YsqLYhsZt"
      },
      "outputs": [],
      "source": [
        "print(classification_report(labels, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gYTthiRimcd"
      },
      "outputs": [],
      "source": [
        "f1 = f1_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYXt7Se0ilAI"
      },
      "outputs": [],
      "source": [
        "if TRACKING:\n",
        "    wandb.log({'test_f1': f1})\n",
        "    wandb.log({'test_recall': recall})\n",
        "    wandb.log({'test_precision': precision})\n",
        "    wandb.log({'test_accuracy': accuracy})\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-kO6Dhlq0fk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "claim_model_fasttext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
